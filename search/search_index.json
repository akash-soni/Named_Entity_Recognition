{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Data%20Ingestion/","text":"Data Ingestion Steps Step-1. Mention the artifacts for data ingestion in config.yaml paths: artifacts: artifacts data_store: data Step-2. Specify the name and subset of data we need for NER task in config.yaml data_ingestion_config: dataset_name: xtreme subset_name: PAN-X.en Step-3. Import all the necessary libraries and also import from datasets import load_dataset Step-4. Initialize class DataIngestion and connect the directory path Step-5. Now we will create a function which will get the data into our data/ directory def get_data ( self ): try : \"\"\" This is class is responsible for data collection from official hugging face library. Cross-lingual Transfer Evaluation of Multilingual Encoders (XTREME) benchmark called WikiANN or PAN-X. Returns: Dict of train test validation data \"\"\" # Task Implement save data to artifacts/data_store , write check if data already exists there # if not then only fetch from load_dataset logger . info ( f \"Loading Data from Hugging face \" ) if not os . path . isdir ( self . data_ingestion_config . data_path ): logger . info ( f \"No data storage directory found creating directory\" ) os . mkdir ( self . data_ingestion_config . data_path ) pan_en_data = load_dataset ( self . data_ingestion_config . dataset_name , name = self . data_ingestion_config . subset_name ) # saving data locally pan_en_data . save_to_disk ( self . data_ingestion_config . data_path ) logger . info ( f \"Dataset loading completed\" ) logger . info ( f \"Dataset Info : { pan_en_data } \" ) else : logger . info ( f \"loading dataset from local disk\" ) # reloading dataset from local drive pan_en_data = load_from_disk ( self . data_ingestion_config . data_path ) logger . info ( f \"loading dataset completed\" ) return pan_en_data Step-6. The data can be observed as follows","title":"Ingestion"},{"location":"Data%20Ingestion/#data-ingestion-steps","text":"Step-1. Mention the artifacts for data ingestion in config.yaml paths: artifacts: artifacts data_store: data Step-2. Specify the name and subset of data we need for NER task in config.yaml data_ingestion_config: dataset_name: xtreme subset_name: PAN-X.en Step-3. Import all the necessary libraries and also import from datasets import load_dataset Step-4. Initialize class DataIngestion and connect the directory path Step-5. Now we will create a function which will get the data into our data/ directory def get_data ( self ): try : \"\"\" This is class is responsible for data collection from official hugging face library. Cross-lingual Transfer Evaluation of Multilingual Encoders (XTREME) benchmark called WikiANN or PAN-X. Returns: Dict of train test validation data \"\"\" # Task Implement save data to artifacts/data_store , write check if data already exists there # if not then only fetch from load_dataset logger . info ( f \"Loading Data from Hugging face \" ) if not os . path . isdir ( self . data_ingestion_config . data_path ): logger . info ( f \"No data storage directory found creating directory\" ) os . mkdir ( self . data_ingestion_config . data_path ) pan_en_data = load_dataset ( self . data_ingestion_config . dataset_name , name = self . data_ingestion_config . subset_name ) # saving data locally pan_en_data . save_to_disk ( self . data_ingestion_config . data_path ) logger . info ( f \"Dataset loading completed\" ) logger . info ( f \"Dataset Info : { pan_en_data } \" ) else : logger . info ( f \"loading dataset from local disk\" ) # reloading dataset from local drive pan_en_data = load_from_disk ( self . data_ingestion_config . data_path ) logger . info ( f \"loading dataset completed\" ) return pan_en_data Step-6. The data can be observed as follows","title":"Data Ingestion Steps"},{"location":"Data%20Preperation/","text":"Data Preperation Steps In this step we will convert strings to numerical tokens and we will also apply necessary preprocessing on data tags Tag Objects Obsererving no. of prediction tags. tags = en [ \"train\" ] . features [ \"ner_tags\" ] . feature print ( tags ) ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None) We have 7 classes: 0 : 'O' 1 : 'B-PER' --> (Entity is begining with person. Ex: Sentence is starting with person) 2 : 'I-PER' --> (Entity have a person in between. Ex: In a sentence a peson is somewhere in the middle) 3 : 'B-ORG' --> (Entity is begining with name of organisation) 4 : 'I-ORG' --> (Entity have organisation somewhere int he middle) 5 : 'B-LOC' --> (Entity is begining with name of Location) 6 : 'I-LOC' --> (Entity have Location somewhere int he middle) Integers to tags Creating tag name for integers and also integers to tags index2tag = { idx : tag for idx , tag in enumerate ( tags . names )} tag2index = { tag : idx for idx , tag in enumerate ( tags . names )} Adding a column \"ner_tags_str\" in all the Train, Test and Validation dataset to get the clear tags names understanding. tags = en [ \"train\" ] . features [ \"ner_tags\" ] . feature print ( tags ) def create_tag_name ( batch ): return { \"ner_tags_str\" :[ tags . int2str ( idx ) for idx in batch [ \"ner_tags\" ]]} # mapping this to all train, test and validation data new_en = en . map ( create_tag_name ) new_en the output appears as follows: DatasetDict({ validation: Dataset({ features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'], num_rows: 10000 }) test: Dataset({ features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'], num_rows: 10000 }) train: Dataset({ features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'], num_rows: 20000 }) }) 'ner_tags' --> [3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0] 'ner_tags_str' --> [B-ORG, I-ORG, O, B-ORG, I-ORG, I-ORG, O, O, O, O, O] Xlmr-Tokenizer XLM-R stands for(XLM)cross language modelling and (R)Roberta is special model for cross entity language modelling XLM-R -> have vocab size 250,000 words Instead of using a WordPiece tokenizer, XLM-R uses a sentence tokenizer called SentencePiece. this tokenizer preserve White spaces using _ . Vocab - After tokenization replace with index position # downloading tokenizer def get_model_and_tokenizer (): from transformers import AutoTokenizer xlmr_model_name = \"xlm-roberta-base\" xlmr_tokenizer = AutoTokenizer . from_pretrained ( xlmr_model_name ) return xlmr_model_name , xlmr_tokenizer xlmr_model_name , xlmr_tokenizer = get_model_and_tokenizer () Tokenizing text of NER Lets pick up some random datapoint from train data and tokenize it using XLMR tokenizer. de_example = new_en [ \"train\" ][ 8 ] words , labels = de_example [ \"tokens\" ], de_example [ \"ner_tags\" ] words , labels (['*Inducted', 'into', 'the', 'United', 'States', 'Hockey', 'Hall', 'of', 'Fame', 'in', '2015'], [0, 0, 0, 3, 4, 4, 4, 4, 4, 0, 0]) # create integer tokens and attention mask from the sentence tokenized_input = xlmr_tokenizer ( de_example [ \"tokens\" ], is_split_into_words = True ) tokenized_input {'input_ids': [0, 661, 4153, 77193, 297, 3934, 70, 14098, 46684, 193171, 19449, 111, 52917, 13, 23, 918, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} # getting word tokens from the integer tokens tokens = xlmr_tokenizer . convert_ids_to_tokens ( tokenized_input [ \"input_ids\" ]) tokens ['<s>', '\u2581*', 'In', 'duct', 'ed', '\u2581into', '\u2581the', '\u2581United', '\u2581States', '\u2581Hockey', '\u2581Hall', '\u2581of', '\u2581Fam', 'e', '\u2581in', '\u25812015', '</s>'] # for each word token we will provide it with the word id. word_ids = tokenized_input . word_ids () word_ids [None, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, None] print ( f \"length of actual sentence: { len ( words ) - 2 } \" ) print ( f \"length of ner_tags/labels : { len ( labels ) - 2 } \" ) print ( f \"length of tokenized words : { len ( tokens ) - 2 } \" ) length of actual sentence: 9 length of ner_tags/labels : 9 length of tokenized words : 15 we can observe that we have got more no. of tokenized words then the actual words as XLMR tokenizer is internally performing some prefix seperation operation notice word \"Inducted\" is tokenized as \" *\", \"In\", \"duct\", \"ed\" and this is 1 complete word without and underscore symbol( ) so this we got 4 zeros, what happens internally is the loop looks for is underscores to understand it is next word and then gives word_id to the next word. so the wordids for inducted is [0,0,0,0] notice word \"Fame\" is tokenized as \"_Fam\", \"e\" so both have got word_id [8,8] Other thing that we can observe here is, instead of spaces XLMR uses \"_\" to denote spaces. Label length synchronization Synchronizing sentence length and tokenized word length. Now the problem in above example is we have actual sentence length of 9 --> labels are also 9(excluding start and end tags) but the no. of tokenized words are 15. # Actual label of the sentence [ index2tag [ idx ] for idx in de_example [ \"ner_tags\" ]][ 1 : - 1 ] ['O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O'] previous_word_idx = None label_ids = [] for word_idx in word_ids : if word_idx is None or word_idx == previous_word_idx : label_ids . append ( - 100 ) elif word_idx != previous_word_idx : label_ids . append ( labels [ word_idx ]) previous_word_idx = word_idx labels = [ index2tag [ l ] if l != - 100 else \"IGN\" for l in label_ids ] index = [ \"Tokens\" , \"Word IDs\" , \"Label IDs\" , \"Labels\" ] pd . DataFrame ([ tokens , word_ids , label_ids , labels ], index = index ) so to resolve the length issue- * we have converted all the tokenized words to their word_ids(). * suppose word \"Inducted\" is tokenized as \"_*\", \"In\", \"duct\", \"ed\" then these tokens will get word_ids 0, 0, 0, 0 * now if after the 1st occurrence of a word_id if the same id repeats then we will give it label_id -100 which is simply for ignoring(IGN). * In this way we will again get the original length labels. Final preprocessing step So now what we need to do? * for model training we do not need \"Tokens\", \"language\" fields as these are only strings so we will remove them. * We will also remove \"ner_tags\", instead of this we will introduce field \"Labels\" which will also have -100 value for each tokenized word. * For field \"ner_tags_str\" we will not do anything, there is no need to append \"IGN(Ignore)\" tag * we will add \"attention mask\", \"input_ids\" along with \"Labels\", \"ner_tags_str\" fields def tokenize_and_align_labels ( examples ): tokenized_inputs = xlmr_tokenizer ( examples [ \"tokens\" ], truncation = True , is_split_into_words = True ) labels = [] for idx , label in enumerate ( examples [ \"ner_tags\" ]): word_ids = tokenized_inputs . word_ids ( batch_index = idx ) previous_word_idx = None label_ids = [] for word_idx in word_ids : if word_idx is None or word_idx == previous_word_idx : label_ids . append ( - 100 ) else : label_ids . append ( label [ word_idx ]) previous_word_idx = word_idx labels . append ( label_ids ) tokenized_inputs [ \"labels\" ] = labels return tokenized_inputs def encode_panx_dataset ( corpus ): return corpus . map ( tokenize_and_align_labels , batched = True , remove_columns = [ 'langs' , 'ner_tags' , 'tokens' ]) panx_en_encoded = encode_panx_dataset ( new_en ) panx_en_encoded DatasetDict({ validation: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'ner_tags_str'], num_rows: 10000 }) test: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'ner_tags_str'], num_rows: 10000 }) train: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'ner_tags_str'], num_rows: 20000 }) }) previous Features: * ['tokens', 'ner_tags', 'langs', 'ner_tags_str'] de_example = new_en [ \"train\" ][ 8 ] pd . DataFrame ([ de_example [ \"tokens\" ], de_example [ \"ner_tags\" ] , de_example [ \"ner_tags_str\" ], de_example [ \"langs\" ]],[ 'Tokens' , \"ner_tags\" , 'ner_tags_str' , 'language' ]) New features: * ['attention_mask', 'input_ids', 'labels', 'ner_tags_str'] de_example = panx_en_encoded [ \"train\" ][ 8 ] print ( \"sentence Tags:\" ) print ( de_example [ 'ner_tags_str' ]) print ( \" \\n \" ) df = pd . DataFrame ([ de_example [ \"attention_mask\" ], de_example [ \"input_ids\" ] , de_example [ \"labels\" ]], [ 'attention_mask' , \"input_ids\" , 'labels' ]) df . head ()","title":"Data Preperation"},{"location":"Data%20Preperation/#data-preperation-steps","text":"In this step we will convert strings to numerical tokens and we will also apply necessary preprocessing on data tags","title":"Data Preperation Steps"},{"location":"Data%20Preperation/#tag-objects","text":"Obsererving no. of prediction tags. tags = en [ \"train\" ] . features [ \"ner_tags\" ] . feature print ( tags ) ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None) We have 7 classes: 0 : 'O' 1 : 'B-PER' --> (Entity is begining with person. Ex: Sentence is starting with person) 2 : 'I-PER' --> (Entity have a person in between. Ex: In a sentence a peson is somewhere in the middle) 3 : 'B-ORG' --> (Entity is begining with name of organisation) 4 : 'I-ORG' --> (Entity have organisation somewhere int he middle) 5 : 'B-LOC' --> (Entity is begining with name of Location) 6 : 'I-LOC' --> (Entity have Location somewhere int he middle)","title":"Tag Objects"},{"location":"Data%20Preperation/#integers-to-tags","text":"Creating tag name for integers and also integers to tags index2tag = { idx : tag for idx , tag in enumerate ( tags . names )} tag2index = { tag : idx for idx , tag in enumerate ( tags . names )} Adding a column \"ner_tags_str\" in all the Train, Test and Validation dataset to get the clear tags names understanding. tags = en [ \"train\" ] . features [ \"ner_tags\" ] . feature print ( tags ) def create_tag_name ( batch ): return { \"ner_tags_str\" :[ tags . int2str ( idx ) for idx in batch [ \"ner_tags\" ]]} # mapping this to all train, test and validation data new_en = en . map ( create_tag_name ) new_en the output appears as follows: DatasetDict({ validation: Dataset({ features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'], num_rows: 10000 }) test: Dataset({ features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'], num_rows: 10000 }) train: Dataset({ features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'], num_rows: 20000 }) }) 'ner_tags' --> [3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0] 'ner_tags_str' --> [B-ORG, I-ORG, O, B-ORG, I-ORG, I-ORG, O, O, O, O, O]","title":"Integers to tags"},{"location":"Data%20Preperation/#xlmr-tokenizer","text":"XLM-R stands for(XLM)cross language modelling and (R)Roberta is special model for cross entity language modelling XLM-R -> have vocab size 250,000 words Instead of using a WordPiece tokenizer, XLM-R uses a sentence tokenizer called SentencePiece. this tokenizer preserve White spaces using _ . Vocab - After tokenization replace with index position # downloading tokenizer def get_model_and_tokenizer (): from transformers import AutoTokenizer xlmr_model_name = \"xlm-roberta-base\" xlmr_tokenizer = AutoTokenizer . from_pretrained ( xlmr_model_name ) return xlmr_model_name , xlmr_tokenizer xlmr_model_name , xlmr_tokenizer = get_model_and_tokenizer ()","title":"Xlmr-Tokenizer"},{"location":"Data%20Preperation/#tokenizing-text-of-ner","text":"Lets pick up some random datapoint from train data and tokenize it using XLMR tokenizer. de_example = new_en [ \"train\" ][ 8 ] words , labels = de_example [ \"tokens\" ], de_example [ \"ner_tags\" ] words , labels (['*Inducted', 'into', 'the', 'United', 'States', 'Hockey', 'Hall', 'of', 'Fame', 'in', '2015'], [0, 0, 0, 3, 4, 4, 4, 4, 4, 0, 0]) # create integer tokens and attention mask from the sentence tokenized_input = xlmr_tokenizer ( de_example [ \"tokens\" ], is_split_into_words = True ) tokenized_input {'input_ids': [0, 661, 4153, 77193, 297, 3934, 70, 14098, 46684, 193171, 19449, 111, 52917, 13, 23, 918, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} # getting word tokens from the integer tokens tokens = xlmr_tokenizer . convert_ids_to_tokens ( tokenized_input [ \"input_ids\" ]) tokens ['<s>', '\u2581*', 'In', 'duct', 'ed', '\u2581into', '\u2581the', '\u2581United', '\u2581States', '\u2581Hockey', '\u2581Hall', '\u2581of', '\u2581Fam', 'e', '\u2581in', '\u25812015', '</s>'] # for each word token we will provide it with the word id. word_ids = tokenized_input . word_ids () word_ids [None, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, None] print ( f \"length of actual sentence: { len ( words ) - 2 } \" ) print ( f \"length of ner_tags/labels : { len ( labels ) - 2 } \" ) print ( f \"length of tokenized words : { len ( tokens ) - 2 } \" ) length of actual sentence: 9 length of ner_tags/labels : 9 length of tokenized words : 15 we can observe that we have got more no. of tokenized words then the actual words as XLMR tokenizer is internally performing some prefix seperation operation notice word \"Inducted\" is tokenized as \" *\", \"In\", \"duct\", \"ed\" and this is 1 complete word without and underscore symbol( ) so this we got 4 zeros, what happens internally is the loop looks for is underscores to understand it is next word and then gives word_id to the next word. so the wordids for inducted is [0,0,0,0] notice word \"Fame\" is tokenized as \"_Fam\", \"e\" so both have got word_id [8,8] Other thing that we can observe here is, instead of spaces XLMR uses \"_\" to denote spaces.","title":"Tokenizing text of NER"},{"location":"Data%20Preperation/#label-length-synchronization","text":"Synchronizing sentence length and tokenized word length. Now the problem in above example is we have actual sentence length of 9 --> labels are also 9(excluding start and end tags) but the no. of tokenized words are 15. # Actual label of the sentence [ index2tag [ idx ] for idx in de_example [ \"ner_tags\" ]][ 1 : - 1 ] ['O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O'] previous_word_idx = None label_ids = [] for word_idx in word_ids : if word_idx is None or word_idx == previous_word_idx : label_ids . append ( - 100 ) elif word_idx != previous_word_idx : label_ids . append ( labels [ word_idx ]) previous_word_idx = word_idx labels = [ index2tag [ l ] if l != - 100 else \"IGN\" for l in label_ids ] index = [ \"Tokens\" , \"Word IDs\" , \"Label IDs\" , \"Labels\" ] pd . DataFrame ([ tokens , word_ids , label_ids , labels ], index = index ) so to resolve the length issue- * we have converted all the tokenized words to their word_ids(). * suppose word \"Inducted\" is tokenized as \"_*\", \"In\", \"duct\", \"ed\" then these tokens will get word_ids 0, 0, 0, 0 * now if after the 1st occurrence of a word_id if the same id repeats then we will give it label_id -100 which is simply for ignoring(IGN). * In this way we will again get the original length labels.","title":"Label length synchronization"},{"location":"Data%20Preperation/#final-preprocessing-step","text":"So now what we need to do? * for model training we do not need \"Tokens\", \"language\" fields as these are only strings so we will remove them. * We will also remove \"ner_tags\", instead of this we will introduce field \"Labels\" which will also have -100 value for each tokenized word. * For field \"ner_tags_str\" we will not do anything, there is no need to append \"IGN(Ignore)\" tag * we will add \"attention mask\", \"input_ids\" along with \"Labels\", \"ner_tags_str\" fields def tokenize_and_align_labels ( examples ): tokenized_inputs = xlmr_tokenizer ( examples [ \"tokens\" ], truncation = True , is_split_into_words = True ) labels = [] for idx , label in enumerate ( examples [ \"ner_tags\" ]): word_ids = tokenized_inputs . word_ids ( batch_index = idx ) previous_word_idx = None label_ids = [] for word_idx in word_ids : if word_idx is None or word_idx == previous_word_idx : label_ids . append ( - 100 ) else : label_ids . append ( label [ word_idx ]) previous_word_idx = word_idx labels . append ( label_ids ) tokenized_inputs [ \"labels\" ] = labels return tokenized_inputs def encode_panx_dataset ( corpus ): return corpus . map ( tokenize_and_align_labels , batched = True , remove_columns = [ 'langs' , 'ner_tags' , 'tokens' ]) panx_en_encoded = encode_panx_dataset ( new_en ) panx_en_encoded DatasetDict({ validation: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'ner_tags_str'], num_rows: 10000 }) test: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'ner_tags_str'], num_rows: 10000 }) train: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'ner_tags_str'], num_rows: 20000 }) }) previous Features: * ['tokens', 'ner_tags', 'langs', 'ner_tags_str'] de_example = new_en [ \"train\" ][ 8 ] pd . DataFrame ([ de_example [ \"tokens\" ], de_example [ \"ner_tags\" ] , de_example [ \"ner_tags_str\" ], de_example [ \"langs\" ]],[ 'Tokens' , \"ner_tags\" , 'ner_tags_str' , 'language' ]) New features: * ['attention_mask', 'input_ids', 'labels', 'ner_tags_str'] de_example = panx_en_encoded [ \"train\" ][ 8 ] print ( \"sentence Tags:\" ) print ( de_example [ 'ner_tags_str' ]) print ( \" \\n \" ) df = pd . DataFrame ([ de_example [ \"attention_mask\" ], de_example [ \"input_ids\" ] , de_example [ \"labels\" ]], [ 'attention_mask' , \"input_ids\" , 'labels' ]) df . head ()","title":"Final preprocessing step"},{"location":"Data%20Validation/","text":"Data Validation Steps Validation Checks For Data Validation we need to check the following: No. of columns must be same in train, test and Validation. Type check on columns Checking if there are some NULL values or not in the dataset Column Length check Lets get the column length for each column col_names = [ \"tokens\" , \"ner_tags\" , \"langs\" ] splits = [ \"train\" , \"test\" , \"validation\" ] result = list () for split in splits : result . append ( sum ( pd . DataFrame ( en_dict [ split ]) . columns == col_names ) ) result The output of the above block is [3,3,3] Sum of Columns names obtained must be equal to 9 if sum ( result ) == len ( col_names ) * len ( splits ): checks_results . append ( True ) else : checks_results . append ( True ) Column Type check Here we are validating columns types * \"tokens\" must be of type \"string\" * \"ner_tags\" must of type \"int64\" * \"langs\" must of type \"string\" splits = [ \"train\" , \"test\" , \"validation\" ] col_names = [ \"tokens\" , \"langs\" , \"ner_tags\" ] types = [ \"string\" , \"int64\" ] result = list () for split in splits : count = 0 for col_name in col_names : if ( en_dict [ split ] . features [ col_name ] . feature . dtype in types ): count += 1 result . append ( count ) print ( \"/n\" ) print ( result ) The output of the above appears as follows: [3, 3, 3] NULL value check for NULL values we are checking Train, Test and Validation. If there are no NULL values we get a list of [False, False, False]. If all values are False we return True else some of data contains NULL value return False pd . DataFrame ( en [ \"train\" ]) . isnull () . values . any () pd . DataFrame ( en [ \"test\" ]) . isnull () . values . any () pd . DataFrame ( en [ \"validation\" ]) . isnull () . values . any () lst = [ False , False , False ] if sum ( lst ) == 0 : print ( True ) else : print ( False )","title":"Validation"},{"location":"Data%20Validation/#data-validation-steps","text":"","title":"Data Validation Steps"},{"location":"Data%20Validation/#validation-checks","text":"For Data Validation we need to check the following: No. of columns must be same in train, test and Validation. Type check on columns Checking if there are some NULL values or not in the dataset","title":"Validation Checks"},{"location":"Data%20Validation/#column-length-check","text":"Lets get the column length for each column col_names = [ \"tokens\" , \"ner_tags\" , \"langs\" ] splits = [ \"train\" , \"test\" , \"validation\" ] result = list () for split in splits : result . append ( sum ( pd . DataFrame ( en_dict [ split ]) . columns == col_names ) ) result The output of the above block is [3,3,3] Sum of Columns names obtained must be equal to 9 if sum ( result ) == len ( col_names ) * len ( splits ): checks_results . append ( True ) else : checks_results . append ( True )","title":"Column Length check"},{"location":"Data%20Validation/#column-type-check","text":"Here we are validating columns types * \"tokens\" must be of type \"string\" * \"ner_tags\" must of type \"int64\" * \"langs\" must of type \"string\" splits = [ \"train\" , \"test\" , \"validation\" ] col_names = [ \"tokens\" , \"langs\" , \"ner_tags\" ] types = [ \"string\" , \"int64\" ] result = list () for split in splits : count = 0 for col_name in col_names : if ( en_dict [ split ] . features [ col_name ] . feature . dtype in types ): count += 1 result . append ( count ) print ( \"/n\" ) print ( result ) The output of the above appears as follows: [3, 3, 3]","title":"Column Type check"},{"location":"Data%20Validation/#null-value-check","text":"for NULL values we are checking Train, Test and Validation. If there are no NULL values we get a list of [False, False, False]. If all values are False we return True else some of data contains NULL value return False pd . DataFrame ( en [ \"train\" ]) . isnull () . values . any () pd . DataFrame ( en [ \"test\" ]) . isnull () . values . any () pd . DataFrame ( en [ \"validation\" ]) . isnull () . values . any () lst = [ False , False , False ] if sum ( lst ) == 0 : print ( True ) else : print ( False )","title":"NULL value check"},{"location":"Model%20Training/","text":"Model Training Steps Initializing RoBERTa Model Body we will use RoBERTa as the base model but augmented with settings specific to XLM-R. The config_class ensures that the standard XLM-R settings are used when we initialize a new model. Note that we set add_\u200bpool\u2060ing_layer=False to ensure all hidden states are returned and not only the one associated with the [CLS] token. Finally, we initialize all the weights by calling the init_weights() import torch.nn as nn from transformers import XLMRobertaConfig # this class will get every model configuration settings of roberta model from transformers.modeling_outputs import TokenClassifierOutput from transformers.models.roberta.modeling_roberta import RobertaModel from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel class XLMRobertaForTokenClassification ( RobertaPreTrainedModel ): config_class = XLMRobertaConfig # notice : # XLMRobertaConfig is the configuration on which the roboerta is pretrained # config is the configuration we modify using Autoconfig for fine tuning the model def __init__ ( self , config ): super () . __init__ ( config ) self . num_labels = config . num_labels #Load model body self . roberta = RobertaModel ( config , add_pooling_layer = False ) # setup token classification head self . dropout = nn . Dropout ( config . hidden_dropout_prob ) self . classifier = nn . Linear ( config . hidden_size , config . num_labels ) # load and initialize weights/ pretrained of roberta model # init_weights() belongs to RobertaPreTrainedModel class which we are inheriting in the __init__ constructor self . init_weights () def forward ( self , input_ids = None , attention_mask = None , token_type_ids = None , labels = None , ** kwargs ): # use model body to get encoder representations outputs = self . roberta ( input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids , ** kwargs ) # Apply classifire to encoder representation sequence_output = self . dropout ( outputs [ 0 ]) logits = self . classifier ( sequence_output ) # calculate losses loss = None if labels is not None : loss_fct = nn . CrossEntropyLoss () loss = loss_fct ( logits . view ( - 1 , self . num_labels ), labels . view ( - 1 )) # Return model output object return TokenClassifierOutput ( loss = loss , logits = logits , hidden_states = outputs . hidden_states , attentions = outputs . attentions ) Fine tuning For the purpose of fine tuning we will use AutoConfig to fine tune the model according to our data. from transformers import AutoConfig xlmr_model_name , xlmr_tokenizer = get_model_and_tokenizer () xlmr_config = AutoConfig . from_pretrained ( xlmr_model_name , num_labels = tags . num_classes , id2label = index2tag , label2id = tag2index ) xlmr_config XLMRobertaConfig { \"architectures\": [ \"XLMRobertaForMaskedLM\" ], \"attention_probs_dropout_prob\": 0.1, \"bos_token_id\": 0, \"classifier_dropout\": null, \"eos_token_id\": 2, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 768, \"id2label\": { \"0\": \"O\", \"1\": \"B-PER\", \"2\": \"I-PER\", \"3\": \"B-ORG\", \"4\": \"I-ORG\", \"5\": \"B-LOC\", \"6\": \"I-LOC\" }, \"initializer_range\": 0.02, \"intermediate_size\": 3072, \"label2id\": { \"B-LOC\": 5, \"B-ORG\": 3, ... \"transformers_version\": \"4.11.3\", \"type_vocab_size\": 1, \"use_cache\": true, \"vocab_size\": 250002 } To summarize AutoConfig: For fine-tuning we need to provide the model name, no. of classes, and many other configurations on which we want to tune our custom data. All these configuration information we provide in AutoConfig. Prediction Matrics Evaluating a NER model is similar to evaluating a text classification model, and it is common to report results for PRECISION, RECALL, and F1-SCORE. The only subtlety is that all words of an entity need to be predicted correctly in order for a prediction to be counted as correct. Prediction Label Alignment We already know that the sentence is seperated into tokens by tokenizer, sometimes tokenizer also seperates prefixes from the token, because of this the tags length increases and we have previously handled by appending -100 and lable as IGN Now since we will now be computing loss so we need the length of predicted tags and original tags to be same. Hence we will try to remove all those tags which have -100 as follows. panx_en_encoded [ 'train' ][ 'labels' ][ 10 ] [-100, 0, -100, -100, 0, 3, -100, 4, 0, 3, -100] in our case we have also substituted -100 as IGNORE token so our data may look like [-100, 0, -100, -100, 0, 3, -100, 4, 0, 3, -100] but we need to skip all -100 while predicting, as we will have to compare actual labels with predicted labels After ignoring -100 Actual labels looks like [0, 0, 3, 4, 0, 3] & Predicted label can be [0, 1, 3, 4, 0, 5] so to compare and get the loss out of it we need to remove -100 import numpy as np def align_predictions ( predictions , label_ids ): preds = np . argmax ( predictions , axis = 2 ) batch_size , seq_len = preds . shape labels_list , preds_list = [], [] for batch_idx in range ( batch_size ): example_labels , example_preds = [], [] for seq_idx in range ( seq_len ): # Ignore label IDs = -100 if label_ids [ batch_idx , seq_idx ] != - 100 : example_labels . append ( index2tag [ label_ids [ batch_idx ][ seq_idx ]]) example_preds . append ( index2tag [ preds [ batch_idx ][ seq_idx ]]) labels_list . append ( example_labels ) preds_list . append ( example_preds ) return preds_list , labels_list from seqeval.metrics import f1_score def compute_metrics ( eval_pred ): y_pred , y_true = align_predictions ( eval_pred . predictions , eval_pred . label_ids ) return { \"f1\" : f1_score ( y_true , y_pred )} Prepare arguments for Fine Tuning Once we are done with aligning the predictions we need to prepare training arguments for fine-Tuning. we only change the arguments we are interested into as there are many arguments which are better left for default. from transformers import TrainingArguments num_epochs = 10 batch_size = 24 logging_steps = len ( panx_en_encoded [ \"train\" ] . select ( range ( 100 ))) // batch_size model_name = f \" { xlmr_model_name } -finetuned-panx-en\" training_args = TrainingArguments ( output_dir = model_name , log_level = \"error\" , num_train_epochs = num_epochs , per_device_train_batch_size = batch_size , per_device_eval_batch_size = batch_size , evaluation_strategy = \"epoch\" , save_steps = 1e6 , weight_decay = 0.01 , disable_tqdm = False , logging_steps = logging_steps ) Data Collator The final step is to define a data collator so we can pad each input sequence to the largest sequence length in a batch. Transformers provides a dedicated data collator for token classification that will pad the labels along with the inputs: from transformers import DataCollatorForTokenClassification data_collator = DataCollatorForTokenClassification ( xlmr_tokenizer ) Training Initializing the model class def model_init (): return ( XLMRobertaForTokenClassification . from_pretrained ( xlmr_model_name , config = xlmr_config ) . to ( device )) Setting up trainer() which will train on the configuration we have set from transformers import Trainer trainer = Trainer ( model_init = model_init , args = training_args , data_collator = data_collator , compute_metrics = compute_metrics , train_dataset = panx_en_encoded [ \"train\" ] . select ( range ( 1000 )), eval_dataset = panx_en_encoded [ \"validation\" ] . select ( range ( 100 )), tokenizer = xlmr_tokenizer ) Start the training trainer . train () Once the training is complete, save the finetuned weights. trainer . save_model ( \"./artifacts/model_weights\" )","title":"Model Training"},{"location":"Model%20Training/#model-training-steps","text":"","title":"Model Training Steps"},{"location":"Model%20Training/#initializing-roberta-model-body","text":"we will use RoBERTa as the base model but augmented with settings specific to XLM-R. The config_class ensures that the standard XLM-R settings are used when we initialize a new model. Note that we set add_\u200bpool\u2060ing_layer=False to ensure all hidden states are returned and not only the one associated with the [CLS] token. Finally, we initialize all the weights by calling the init_weights() import torch.nn as nn from transformers import XLMRobertaConfig # this class will get every model configuration settings of roberta model from transformers.modeling_outputs import TokenClassifierOutput from transformers.models.roberta.modeling_roberta import RobertaModel from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel class XLMRobertaForTokenClassification ( RobertaPreTrainedModel ): config_class = XLMRobertaConfig # notice : # XLMRobertaConfig is the configuration on which the roboerta is pretrained # config is the configuration we modify using Autoconfig for fine tuning the model def __init__ ( self , config ): super () . __init__ ( config ) self . num_labels = config . num_labels #Load model body self . roberta = RobertaModel ( config , add_pooling_layer = False ) # setup token classification head self . dropout = nn . Dropout ( config . hidden_dropout_prob ) self . classifier = nn . Linear ( config . hidden_size , config . num_labels ) # load and initialize weights/ pretrained of roberta model # init_weights() belongs to RobertaPreTrainedModel class which we are inheriting in the __init__ constructor self . init_weights () def forward ( self , input_ids = None , attention_mask = None , token_type_ids = None , labels = None , ** kwargs ): # use model body to get encoder representations outputs = self . roberta ( input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids , ** kwargs ) # Apply classifire to encoder representation sequence_output = self . dropout ( outputs [ 0 ]) logits = self . classifier ( sequence_output ) # calculate losses loss = None if labels is not None : loss_fct = nn . CrossEntropyLoss () loss = loss_fct ( logits . view ( - 1 , self . num_labels ), labels . view ( - 1 )) # Return model output object return TokenClassifierOutput ( loss = loss , logits = logits , hidden_states = outputs . hidden_states , attentions = outputs . attentions )","title":"Initializing RoBERTa Model Body"},{"location":"Model%20Training/#fine-tuning","text":"For the purpose of fine tuning we will use AutoConfig to fine tune the model according to our data. from transformers import AutoConfig xlmr_model_name , xlmr_tokenizer = get_model_and_tokenizer () xlmr_config = AutoConfig . from_pretrained ( xlmr_model_name , num_labels = tags . num_classes , id2label = index2tag , label2id = tag2index ) xlmr_config XLMRobertaConfig { \"architectures\": [ \"XLMRobertaForMaskedLM\" ], \"attention_probs_dropout_prob\": 0.1, \"bos_token_id\": 0, \"classifier_dropout\": null, \"eos_token_id\": 2, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 768, \"id2label\": { \"0\": \"O\", \"1\": \"B-PER\", \"2\": \"I-PER\", \"3\": \"B-ORG\", \"4\": \"I-ORG\", \"5\": \"B-LOC\", \"6\": \"I-LOC\" }, \"initializer_range\": 0.02, \"intermediate_size\": 3072, \"label2id\": { \"B-LOC\": 5, \"B-ORG\": 3, ... \"transformers_version\": \"4.11.3\", \"type_vocab_size\": 1, \"use_cache\": true, \"vocab_size\": 250002 } To summarize AutoConfig: For fine-tuning we need to provide the model name, no. of classes, and many other configurations on which we want to tune our custom data. All these configuration information we provide in AutoConfig.","title":"Fine tuning"},{"location":"Model%20Training/#prediction-matrics","text":"Evaluating a NER model is similar to evaluating a text classification model, and it is common to report results for PRECISION, RECALL, and F1-SCORE. The only subtlety is that all words of an entity need to be predicted correctly in order for a prediction to be counted as correct.","title":"Prediction Matrics"},{"location":"Model%20Training/#prediction-label-alignment","text":"We already know that the sentence is seperated into tokens by tokenizer, sometimes tokenizer also seperates prefixes from the token, because of this the tags length increases and we have previously handled by appending -100 and lable as IGN Now since we will now be computing loss so we need the length of predicted tags and original tags to be same. Hence we will try to remove all those tags which have -100 as follows. panx_en_encoded [ 'train' ][ 'labels' ][ 10 ] [-100, 0, -100, -100, 0, 3, -100, 4, 0, 3, -100] in our case we have also substituted -100 as IGNORE token so our data may look like [-100, 0, -100, -100, 0, 3, -100, 4, 0, 3, -100] but we need to skip all -100 while predicting, as we will have to compare actual labels with predicted labels After ignoring -100 Actual labels looks like [0, 0, 3, 4, 0, 3] & Predicted label can be [0, 1, 3, 4, 0, 5] so to compare and get the loss out of it we need to remove -100 import numpy as np def align_predictions ( predictions , label_ids ): preds = np . argmax ( predictions , axis = 2 ) batch_size , seq_len = preds . shape labels_list , preds_list = [], [] for batch_idx in range ( batch_size ): example_labels , example_preds = [], [] for seq_idx in range ( seq_len ): # Ignore label IDs = -100 if label_ids [ batch_idx , seq_idx ] != - 100 : example_labels . append ( index2tag [ label_ids [ batch_idx ][ seq_idx ]]) example_preds . append ( index2tag [ preds [ batch_idx ][ seq_idx ]]) labels_list . append ( example_labels ) preds_list . append ( example_preds ) return preds_list , labels_list from seqeval.metrics import f1_score def compute_metrics ( eval_pred ): y_pred , y_true = align_predictions ( eval_pred . predictions , eval_pred . label_ids ) return { \"f1\" : f1_score ( y_true , y_pred )}","title":"Prediction Label Alignment"},{"location":"Model%20Training/#prepare-arguments-for-fine-tuning","text":"Once we are done with aligning the predictions we need to prepare training arguments for fine-Tuning. we only change the arguments we are interested into as there are many arguments which are better left for default. from transformers import TrainingArguments num_epochs = 10 batch_size = 24 logging_steps = len ( panx_en_encoded [ \"train\" ] . select ( range ( 100 ))) // batch_size model_name = f \" { xlmr_model_name } -finetuned-panx-en\" training_args = TrainingArguments ( output_dir = model_name , log_level = \"error\" , num_train_epochs = num_epochs , per_device_train_batch_size = batch_size , per_device_eval_batch_size = batch_size , evaluation_strategy = \"epoch\" , save_steps = 1e6 , weight_decay = 0.01 , disable_tqdm = False , logging_steps = logging_steps )","title":"Prepare arguments for Fine Tuning"},{"location":"Model%20Training/#data-collator","text":"The final step is to define a data collator so we can pad each input sequence to the largest sequence length in a batch. Transformers provides a dedicated data collator for token classification that will pad the labels along with the inputs: from transformers import DataCollatorForTokenClassification data_collator = DataCollatorForTokenClassification ( xlmr_tokenizer )","title":"Data Collator"},{"location":"Model%20Training/#training","text":"Initializing the model class def model_init (): return ( XLMRobertaForTokenClassification . from_pretrained ( xlmr_model_name , config = xlmr_config ) . to ( device )) Setting up trainer() which will train on the configuration we have set from transformers import Trainer trainer = Trainer ( model_init = model_init , args = training_args , data_collator = data_collator , compute_metrics = compute_metrics , train_dataset = panx_en_encoded [ \"train\" ] . select ( range ( 1000 )), eval_dataset = panx_en_encoded [ \"validation\" ] . select ( range ( 100 )), tokenizer = xlmr_tokenizer ) Start the training trainer . train () Once the training is complete, save the finetuned weights. trainer . save_model ( \"./artifacts/model_weights\" )","title":"Training"},{"location":"Prediction/","text":"Prediction Steps Loading Model We will need the original architecture. We will need Autoconfig as we will have to provide configuration. We will need model the path of directory were we have stored weights of our fine tuned models. xlmr_fine_model = ( XLMRobertaForTokenClassification . from_pretrained ( \"./artifacts/model_weights\" , config = xlmr_config ) . to ( device )) xlmr_fine_model XLMRobertaForTokenClassification( (roberta): RobertaModel( (embeddings): RobertaEmbeddings( (word_embeddings): Embedding(250002, 768, padding_idx=1) (position_embeddings): Embedding(514, 768, padding_idx=1) (token_type_embeddings): Embedding(1, 768) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): RobertaEncoder( (layer): ModuleList( (0): RobertaLayer( (attention): RobertaAttention( (self): RobertaSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): RobertaSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ... ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=7, bias=True) ) Observer that the last two layers are the ones we have added Prediction Example 1 datapoint = panx_en_encoded [ \"train\" ][ \"input_ids\" ][ 15 ] actual_tags = panx_en_encoded [ \"train\" ][ \"ner_tags_str\" ][ 15 ] print ( datapoint ) tokenized_form = xlmr_tokenizer . convert_ids_to_tokens ( datapoint ) actual_form = xlmr_tokenizer . convert_tokens_to_string ( tokenized_form ) print ( f \"Tokenized form \\n { tokenized_form } \" ) print ( f \"Length of Tokenized form -> { len ( tokenized_form ) } \\n \" ) print ( f \"Actual sentence \\n { actual_form } \" ) print ( f \"Length of actual sentence -> { len ( actual_form . split ()) + 1 } \\n \" ) print ( f \"ACTUAL TAGS \\n { actual_tags } \" ) [0, 54041, 24748, 36216, 6, 4, 51978, 111, 166207, 3956, 136, 147202, 46542, 2] Tokenized form ['<s>', '\u2581Prince', '\u2581Albert', '\u2581Victor', '\u2581', ',', '\u2581Duke', '\u2581of', '\u2581Clare', 'nce', '\u2581and', '\u2581Avon', 'dale', '</s>'] Length of Tokenized form -> 14 Actual sentence <s> Prince Albert Victor , Duke of Clarence and Avondale</s> Length of actual sentence -> 11 ACTUAL TAGS ['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER'] # convert input_ids into tokens data = torch . tensor ( datapoint ) print ( data ) data = data . reshape ( 1 , - 1 ) print ( data ) # applying predictions # prediction without using fine tuned model outputs = xlmr_fine_model ( data . to ( device )) . logits predictions = torch . argmax ( outputs , dim =- 1 ) print ( f \" \\n Number of tokens in sequence: { len ( data [ 0 ]) } \" ) print ( f \"Shape of outputs: { outputs . shape } \" ) print ( outputs ) print ( \" \\n\\n PREDICTED TAGS\" ) pred_tags = [ index2tag [ i . item ()] for i in predictions [ 0 ][ 1 : - 1 ]] print ( pred_tags ) print ( f \"ACTUAL TAGS \\n { actual_tags } \" ) tensor([ 0, 54041, 24748, 36216, 6, 4, 51978, 111, 166207, 3956, 136, 147202, 46542, 2]) tensor([[ 0, 54041, 24748, 36216, 6, 4, 51978, 111, 166207, 3956, 136, 147202, 46542, 2]]) Number of tokens in sequence: 14 Shape of outputs: torch.Size([1, 14, 7]) tensor([[[-2.2180, 2.1255, 3.5564, -0.7727, -0.2437, -1.4792, -1.5153], [-2.0184, 6.7363, 1.1276, 1.1878, -2.6170, -0.6715, -3.0794], [-2.1819, 0.1121, 7.0130, -2.0592, 0.3661, -2.3843, -0.6578], [-2.1361, -0.0949, 7.0442, -2.1551, 0.3254, -2.4463, -0.5389], [-1.4781, 0.0678, 6.8940, -1.9208, 0.1178, -2.4237, -0.7508], [-1.5920, 0.3513, 6.9473, -1.9821, -0.0104, -2.4431, -0.8644], [-2.3219, 0.8577, 6.8792, -1.6826, 0.1418, -2.2356, -0.9913], [-1.8488, -0.0159, 6.9235, -2.0291, 0.4430, -2.2784, -0.5683], [-2.1409, -0.1114, 7.1632, -1.9624, 0.3735, -2.1941, -0.4957], [-2.1326, -0.0247, 7.0479, -2.0179, 0.2779, -2.3607, -0.6154], [-1.8979, -0.3404, 6.9651, -2.0806, 0.4059, -2.4631, -0.5563], [-2.1506, -0.5543, 7.1661, -2.0361, 0.3811, -2.1760, -0.3022], [-2.1761, -0.2058, 7.0206, -2.0092, 0.3562, -2.3475, -0.5382], [ 2.4870, -0.6302, 4.9215, -2.0654, -0.3501, -2.7720, -1.2476]]], device='cuda:0', grad_fn=<ViewBackward0>) PREDICTED TAGS ['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER'] ACTUAL TAGS ['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER'] Removing Redundant predicted tags In above predictions threre is a problem, for every token string even if it is not starting with the underscore, we are labelling it. We can associate -100 to non underscores tokens and where ever we detct -100 we ignore the predicted label, so our predicion outputs will be aligned. Let's do it.... text = 'Alex went to Imax to watch RRR Movie ' tokenized_input = xlmr_tokenizer ( text . split (), is_split_into_words = True ) tokens = xlmr_tokenizer . convert_ids_to_tokens ( tokenized_input [ \"input_ids\" ]) print ( tokens ) word_ids = tokenized_input . word_ids () print ( word_ids ) # convert input_ids into tokens data = torch . tensor ( tokenized_input [ 'input_ids' ]) data = data . reshape ( 1 , - 1 ) outputs = xlmr_fine_model ( data . to ( device )) . logits predictions = torch . argmax ( outputs , dim =- 1 ) print ( predictions [ 0 ]) ['<s>', '\u2581Alex', '\u2581went', '\u2581to', '\u2581I', 'max', '\u2581to', '\u2581watch', '\u2581R', 'RR', '\u2581Movie', '</s>'] [None, 0, 1, 2, 3, 3, 4, 5, 6, 6, 7, None] tensor([4, 1, 0, 0, 3, 4, 0, 0, 3, 4, 4, 0]) prediction = [ i . item () for i in predictions [ 0 ]] previous_word_idx = None pred_ids = [] for idx , word_idx in enumerate ( word_ids ): if word_idx is None or word_idx == previous_word_idx : continue elif word_idx != previous_word_idx : pred_ids . append ( prediction [ idx ]) previous_word_idx = word_idx pred_ids [1, 0, 0, 3, 0, 0, 3, 4] [ index2tag [ idx ] for idx in pred_ids ] ['B-PER', 'O', 'O', 'B-ORG', 'O', 'O', 'B-ORG', 'I-ORG']","title":"Prediction"},{"location":"Prediction/#prediction-steps","text":"","title":"Prediction Steps"},{"location":"Prediction/#loading-model","text":"We will need the original architecture. We will need Autoconfig as we will have to provide configuration. We will need model the path of directory were we have stored weights of our fine tuned models. xlmr_fine_model = ( XLMRobertaForTokenClassification . from_pretrained ( \"./artifacts/model_weights\" , config = xlmr_config ) . to ( device )) xlmr_fine_model XLMRobertaForTokenClassification( (roberta): RobertaModel( (embeddings): RobertaEmbeddings( (word_embeddings): Embedding(250002, 768, padding_idx=1) (position_embeddings): Embedding(514, 768, padding_idx=1) (token_type_embeddings): Embedding(1, 768) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): RobertaEncoder( (layer): ModuleList( (0): RobertaLayer( (attention): RobertaAttention( (self): RobertaSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): RobertaSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ... ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=7, bias=True) ) Observer that the last two layers are the ones we have added","title":"Loading Model"},{"location":"Prediction/#prediction-example-1","text":"datapoint = panx_en_encoded [ \"train\" ][ \"input_ids\" ][ 15 ] actual_tags = panx_en_encoded [ \"train\" ][ \"ner_tags_str\" ][ 15 ] print ( datapoint ) tokenized_form = xlmr_tokenizer . convert_ids_to_tokens ( datapoint ) actual_form = xlmr_tokenizer . convert_tokens_to_string ( tokenized_form ) print ( f \"Tokenized form \\n { tokenized_form } \" ) print ( f \"Length of Tokenized form -> { len ( tokenized_form ) } \\n \" ) print ( f \"Actual sentence \\n { actual_form } \" ) print ( f \"Length of actual sentence -> { len ( actual_form . split ()) + 1 } \\n \" ) print ( f \"ACTUAL TAGS \\n { actual_tags } \" ) [0, 54041, 24748, 36216, 6, 4, 51978, 111, 166207, 3956, 136, 147202, 46542, 2] Tokenized form ['<s>', '\u2581Prince', '\u2581Albert', '\u2581Victor', '\u2581', ',', '\u2581Duke', '\u2581of', '\u2581Clare', 'nce', '\u2581and', '\u2581Avon', 'dale', '</s>'] Length of Tokenized form -> 14 Actual sentence <s> Prince Albert Victor , Duke of Clarence and Avondale</s> Length of actual sentence -> 11 ACTUAL TAGS ['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER'] # convert input_ids into tokens data = torch . tensor ( datapoint ) print ( data ) data = data . reshape ( 1 , - 1 ) print ( data ) # applying predictions # prediction without using fine tuned model outputs = xlmr_fine_model ( data . to ( device )) . logits predictions = torch . argmax ( outputs , dim =- 1 ) print ( f \" \\n Number of tokens in sequence: { len ( data [ 0 ]) } \" ) print ( f \"Shape of outputs: { outputs . shape } \" ) print ( outputs ) print ( \" \\n\\n PREDICTED TAGS\" ) pred_tags = [ index2tag [ i . item ()] for i in predictions [ 0 ][ 1 : - 1 ]] print ( pred_tags ) print ( f \"ACTUAL TAGS \\n { actual_tags } \" ) tensor([ 0, 54041, 24748, 36216, 6, 4, 51978, 111, 166207, 3956, 136, 147202, 46542, 2]) tensor([[ 0, 54041, 24748, 36216, 6, 4, 51978, 111, 166207, 3956, 136, 147202, 46542, 2]]) Number of tokens in sequence: 14 Shape of outputs: torch.Size([1, 14, 7]) tensor([[[-2.2180, 2.1255, 3.5564, -0.7727, -0.2437, -1.4792, -1.5153], [-2.0184, 6.7363, 1.1276, 1.1878, -2.6170, -0.6715, -3.0794], [-2.1819, 0.1121, 7.0130, -2.0592, 0.3661, -2.3843, -0.6578], [-2.1361, -0.0949, 7.0442, -2.1551, 0.3254, -2.4463, -0.5389], [-1.4781, 0.0678, 6.8940, -1.9208, 0.1178, -2.4237, -0.7508], [-1.5920, 0.3513, 6.9473, -1.9821, -0.0104, -2.4431, -0.8644], [-2.3219, 0.8577, 6.8792, -1.6826, 0.1418, -2.2356, -0.9913], [-1.8488, -0.0159, 6.9235, -2.0291, 0.4430, -2.2784, -0.5683], [-2.1409, -0.1114, 7.1632, -1.9624, 0.3735, -2.1941, -0.4957], [-2.1326, -0.0247, 7.0479, -2.0179, 0.2779, -2.3607, -0.6154], [-1.8979, -0.3404, 6.9651, -2.0806, 0.4059, -2.4631, -0.5563], [-2.1506, -0.5543, 7.1661, -2.0361, 0.3811, -2.1760, -0.3022], [-2.1761, -0.2058, 7.0206, -2.0092, 0.3562, -2.3475, -0.5382], [ 2.4870, -0.6302, 4.9215, -2.0654, -0.3501, -2.7720, -1.2476]]], device='cuda:0', grad_fn=<ViewBackward0>) PREDICTED TAGS ['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER'] ACTUAL TAGS ['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER']","title":"Prediction Example 1"},{"location":"Prediction/#removing-redundant-predicted-tags","text":"In above predictions threre is a problem, for every token string even if it is not starting with the underscore, we are labelling it. We can associate -100 to non underscores tokens and where ever we detct -100 we ignore the predicted label, so our predicion outputs will be aligned. Let's do it.... text = 'Alex went to Imax to watch RRR Movie ' tokenized_input = xlmr_tokenizer ( text . split (), is_split_into_words = True ) tokens = xlmr_tokenizer . convert_ids_to_tokens ( tokenized_input [ \"input_ids\" ]) print ( tokens ) word_ids = tokenized_input . word_ids () print ( word_ids ) # convert input_ids into tokens data = torch . tensor ( tokenized_input [ 'input_ids' ]) data = data . reshape ( 1 , - 1 ) outputs = xlmr_fine_model ( data . to ( device )) . logits predictions = torch . argmax ( outputs , dim =- 1 ) print ( predictions [ 0 ]) ['<s>', '\u2581Alex', '\u2581went', '\u2581to', '\u2581I', 'max', '\u2581to', '\u2581watch', '\u2581R', 'RR', '\u2581Movie', '</s>'] [None, 0, 1, 2, 3, 3, 4, 5, 6, 6, 7, None] tensor([4, 1, 0, 0, 3, 4, 0, 0, 3, 4, 4, 0]) prediction = [ i . item () for i in predictions [ 0 ]] previous_word_idx = None pred_ids = [] for idx , word_idx in enumerate ( word_ids ): if word_idx is None or word_idx == previous_word_idx : continue elif word_idx != previous_word_idx : pred_ids . append ( prediction [ idx ]) previous_word_idx = word_idx pred_ids [1, 0, 0, 3, 0, 0, 3, 4] [ index2tag [ idx ] for idx in pred_ids ] ['B-PER', 'O', 'O', 'B-ORG', 'O', 'O', 'B-ORG', 'I-ORG']","title":"Removing Redundant predicted tags"}]}